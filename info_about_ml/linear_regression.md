# Информация о линейной регрессии

## Содержание
1. [Основные понятия](#основные-понятия)
2. [Математическая модель](#математическая-модель)
3. [Метод наименьших квадратов](#метод-наименьших-квадратов)
4. [Решение в матричной форме](#решение-в-матричной-форме)
5. [Градиентный спуск](#градиентный-спуск)
6. [Метрики качества](#метрики-качества)
7. [Регуляризация](#регуляризация)
8. [Практические аспекты](#практические-аспекты)
9. [Проверка предположений](#проверка-предположений)
10. [Анализ и интерпретация результатов](#анализ-и-интерпретация-результатов)
11. [Бутстрап и доверительные интервалы](#бутстрап-и-доверительные-интервалы)

---

## Основные понятия

### Что такое линейная регрессия?
Линейная регрессия — статистический метод моделирования зависимости между:
- **Объясняющими переменными** (признаки, факторы) `x_1, x_2, ..., x_n`
- **Зависимой переменной** (целевая переменная) `y`

### Типы линейной регрессии
- **Простая линейная регрессия**: одна независимая переменная
- **Множественная линейная регрессия**: несколько независимых переменных

### Геометрическая интерпретация
Для случая с одним признаком модель пытается найти прямую линию, которая наилучшим образом аппроксимирует облако точек данных в пространстве признаков.

---

## Математическая модель

### Общая формула
**`ŷ = w₀ + w₁x₁ + w₂x₂ + … + wₙxₙ`**

где:
- `ŷ` — предсказанное значение целевой переменной
- `w₀` — свободный член (intercept)
- `w₁, w₂, ..., wₙ` — коэффициенты при признаках
- `x₁, x₂, ..., xₙ` — значения признаков

### Векторная форма
**`ŷ = wᵀx`**

где `w = [w₀, w₁, ..., wₙ]ᵀ` и `x = [1, x₁, x₂, ..., xₙ]ᵀ` (с добавленной единицей)

---

## Метод наименьших квадратов

### Целевая функция
Минимизация суммы квадратов ошибок:

**`MSE = (1/n) * Σ(yᵢ - ŷᵢ)²`**

### Решение для одного признака
Для модели `ŷ = w₀ + w₁x`:

**`w₁ = [Σ(xᵢ - x̄)(yᵢ - ȳ)] / [Σ(xᵢ - x̄)²]`**

**`w₀ = ȳ - w₁x̄`**

где:
- `x̄` — выборочное среднее признака
- `ȳ` — выборочное среднее целевой переменной

### Статистические свойства оценок
- **Несмещённость**: `E[w] = w_{истинный}`
- **Эффективность**: минимальная дисперсия среди несмещённых оценок
- **Состоятельность**: с ростом объёма данных оценка сходится к истинному значению

---

## Решение в матричной форме

### Матричное представление
**`y = Xw + ε`**

где:
- `y` — вектор целевых значений размерности `n × 1`
- `X` — матрица признаков размерности `n × (m+1)`
- `w` — вектор параметров размерности `(m+1) × 1`
- `ε` — вектор ошибок размерности `n × 1`

### Нормальное уравнение
**`w = (XᵀX)⁻¹Xᵀy`**

### Условия существования решения
- Матрица `XᵀX` должна быть обратима
- Признаки не должны быть линейно зависимыми
- Количество наблюдений должно быть больше количества признаков

---

## Градиентный спуск

### Итеративная формула
**`w^{(k+1)} = w^{(k)} - η * ∇Q(w^{(k)})`**

где:
- `η` — скорость обучения (learning rate)
- `∇Q(w)` — градиент функции потерь

### Виды градиентного спуска
1. **Полный градиентный спуск**: использование всех данных для вычисления градиента
2. **Стохастический градиентный спуск (SGD)**: использование одного случайного наблюдения
3. **Mini-batch градиентный спуск**: использование подвыборки данных

### Градиент MSE
**`∇MSE(w) = -(2/n) * Xᵀ(y - Xw)`**

---

## Метрики качества

### Основные метрики

<div align="center">
<table>
  <tr align="center">
    <th>Метрика</th>
    <th>Формула</th>
    <th>Особенности</th>
  </tr>
  <tr align="center">
    <td><strong>MSE</strong></td>
    <td><code>(1/n) * Σ(yᵢ - ŷᵢ)²</code></td>
    <td>Чувствительна к выбросам, дифференцируема</td>
  </tr>
  <tr align="center">
    <td><strong>RMSE</strong></td>
    <td><code>√MSE</code></td>
    <td>Интерпретируема в единицах целевой переменной</td>
  </tr>
  <tr align="center">
    <td><strong>MAE</strong></td>
    <td><code>(1/n) * Σ|yᵢ - ŷᵢ|</code></td>
    <td>Менее чувствительна к выбросам</td>
  </tr>
  <tr align="center">
    <td><strong>R²</strong></td>
    <td><code>1 - [Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²]</code></td>
    <td>Доля объяснённой дисперсии</td>
  </tr>
</table>
</div>

### Дополнительные метрики
- **Adjusted R²**: `1 - [(1-R²)(n-1)/(n-p-1)]`
- **MAPE**: `(100/n) * Σ\|(yᵢ - ŷᵢ)/yᵢ\|`
- **MSLE**: `(1/n) * Σ(log(yᵢ+1) - log(ŷᵢ+1))²`

### Интерпретация R²
- **0.0**: модель не лучше среднего значения
- **0.0-0.3**: слабая объясняющая способность
- **0.3-0.7**: умеренная объясняющая способность
- **0.7-1.0**: сильная объясняющая способность
- **1.0**: идеальное предсказание (обычно признак переобучения)

---

## Регуляризация

### Проблема переобучения
Когда модель слишком сложна и подстраивается под шум в данных.

### Ridge регрессия (L2-регуляризация)
**`Q(w) = MSE + α * Σwᵢ²`**

Решение: **`w = (XᵀX + αI)⁻¹Xᵀy`**

### Lasso регрессия (L1-регуляризация)
**`Q(w) = MSE + α * Σ\|wᵢ\|`**

Приводит к обнулению некоторых коэффициентов

### Elastic Net
Комбинация L1 и L2 регуляризации:
**`Q(w) = MSE + α * ρ * Σ\|wᵢ\| + α * (1-ρ)/2 * Σwᵢ²`**

---

## Практические аспекты

### Предобработка данных
1. **Стандартизация**: `x' = (x - μ)/σ`
2. **Нормализация**: `x' = (x - min)/(max - min)`
3. **Работа с пропущенными значениями**
4. **Обработка выбросов**
5. **Кодирование категориальных переменных**:
   - One-Hot Encoding
   - Target Encoding
   - Label Encoding

### Особенности реализации
```python
# Scikit-learn пример
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Предобработка
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Разделение данных
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)

# Обучение модели
model = LinearRegression()
model.fit(X_train, y_train)

# Предсказание
y_pred = model.predict(X_test)
```

### Анализ данных перед моделированием
1. Исследование распределения целевой переменной
2. Анализ корреляций между признаками и целевой переменной
3. Визуализация взаимосвязей (scatter plots, box plots)
4. Исследование категориальных переменных через анализ средних значений по группам

---

## Проверка предположений

### Линейность
Зависимость между признаками и целевой переменной должна быть линейной.
**Проверка**: анализ остатков (residuals vs fitted plot)

### Гомоскедастичность
Постоянство дисперсии ошибок.
**Проверка**: анализ остатков (равномерное распределение точек)

### Нормальность распределения ошибок
Ошибки должны быть нормально распределены.
**Проверка**: Q-Q plot, тест Шапиро-Уилка

### Отсутствие мультиколлинеарности
Признаки не должны быть сильно коррелированы между собой.
**Проверка**: матрица корреляций, VIF (Variance Inflation Factor)

### Независимость ошибок
Ошибки не должны быть коррелированы между собой.
**Проверка**: тест Дарбина-Уотсона

---

## Анализ и интерпретация результатов

### Анализ коэффициентов
- **Знак коэффициента**: направление влияния признака на целевую переменную
- **Величина коэффициента**: сила влияния (при стандартизированных признаках)
- **Статистическая значимость**: p-value < 0.05

### Важность признаков
Сортировка признаков по абсолютному значению коэффициентов:
```python
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': model.coef_,
    'abs_coefficient': np.abs(model.coef_)
}).sort_values('abs_coefficient', ascending=False)
```

### Анализ остатков
- **График остатков**: остатки vs предсказанные значения
- **Распределение остатков**: должно быть нормальным
- **Выбросы**: точки с большими остатками

---

## Бутстрап и доверительные интервалы

### Метод бутстрапа
Статистический метод для оценки неопределенности метрик через многократное ресэмплирование данных.

### Реализация бутстрапа
```python
from sklearn.utils import resample

n_bootstraps = 1000
confidence_level = 0.95
r2_scores = []

for i in range(n_bootstraps):
    X_boot, y_boot = resample(X_test, y_test, random_state=i)
    y_pred_boot = model.predict(X_boot)
    r2_scores.append(r2_score(y_boot, y_pred_boot))
```

### Расчет доверительных интервалов
```python
def calculate_confidence_interval(scores, confidence=0.95):
    alpha = (1 - confidence) / 2
    lower = np.percentile(scores, alpha * 100)
    upper = np.percentile(scores, (1 - alpha) * 100)
    return lower, upper
```

### Интерпретация доверительных интервалов
- **Узкий интервал**: высокая уверенность в оценке
- **Широкий интервал**: большая неопределенность
- **Включает ноль**: эффект может быть не значим

### Практическая полезность модели
Анализ худшего случая по нижней границе доверительного интервала:
- Минимальный ожидаемый R²
- Максимальные ожидаемые ошибки (MAE, RMSE)
