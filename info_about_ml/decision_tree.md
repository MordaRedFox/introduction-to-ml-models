# Информация о дереве принятия решений и случайном лесе

## Содержание
1. [Основные понятия](#основные-понятия)
2. [Многоклассовая классификация](#многоклассовая-классификация)
3. [Алгоритм построения дерева](#алгоритм-построения-дерева)
4. [Критерии разделения](#критерии-разделения)
5. [Борьба с переобучением](#борьба-с-переобучением)
6. [Случайный лес](#случайный-лес)
7. [Практические аспекты](#практические-аспекты)
8. [Сравнение с линейными моделями](#сравнение-с-линейными-моделями)
9. [Метрики качества](#метрики-качества)
10. [Бутстрап и доверительные интервалы](#бутстрап-и-доверительные-интервалы)
11. [Реализация в проекте](#реализация-в-проекте)
12. [Заключение](#заключение)

---

## Основные понятия

### Что такое дерево принятия решений?
Дерево принятия решений — алгоритм машинного обучения, который строит древовидную структуру решений для классификации или регрессии. Это один из наиболее интуитивно понятных алгоритмов, поскольку он имитирует процесс принятия решений человеком.

### Геометрическая интерпретация
Каждому вопросу вида «значение фактора **`xᵢ ≥ c`**» соответствует полупространство. Пути от корня до листа соответствуют областям пространства, ограниченным гиперплоскостями. Дерево решений по сути разбивает пространство признаков на прямоугольные области, каждая из которых соответствует определенному классу или значению.

### Преимущества дерева решений
- **Интерпретируемость**: легко визуализировать и понимать логику предсказаний
- **Нелинейность**: может моделировать сложные нелинейные зависимости
- **Универсальность**: работает с численными и категориальными признаками
- **Не требует масштабирования**: не чувствителен к масштабу признаков

### Ограничения
- **Склонность к переобучению**: особенно при глубоких деревьях
- **Неустойчивость**: небольшие изменения данных могут значительно изменить структуру дерева
- **Локальная оптимизация**: жадный алгоритм не гарантирует глобального оптимума

---

## Многоклассовая классификация

### Подход "Один против всех" (One-vs-Rest)
- Строим k бинарных классификаторов
- i-й классификатор определяет принадлежность к классу i или остальным
- Выбирается класс с наибольшим значением **`⟨wᵢ,x⟩`**

### Подход "Все против всех" (One-vs-One)
- Для каждой пары классов строим бинарный классификатор
- Выбираем класс с наибольшим количеством "голосов"
- Может возникать неоднозначность при равном количестве голосов

### Метрика Accuracy
**`accuracy = число правильно классифицированных объектов / число всех объектов`**

Базовая метрика, но может быть неинформативной при несбалансированных классах.

### Многоклассовые Precision и Recall
**Micro-averaging:**
- Учитывает общее количество **`TP, FP, FN`** всех классов
- **`precision = ΣTPᵢ / (ΣTPᵢ + ΣFPᵢ)`**
- **`recall = ΣTPᵢ / (ΣTPᵢ + ΣFNᵢ)`**
- Хорош когда важен общий баланс между precision и recall

**Macro-averaging:**
- Усредняет метрики по всем классам
- **`precision = (1/k) * Σprecisionᵢ`**
- **`recall = (1/k) * Σrecallᵢ`**
- Лучше когда все классы одинаково важны

---

## Алгоритм построения дерева

### Рекурсивное разделение
Алгоритм построения дерева использует жадный подход "разделяй и властвуй". На каждом шаге выбирается наилучшее разделение данных по одному из признаков.

Пример:

```python
def build_decision_tree(x, current_depth):
    # Базовые случаи - условия остановки рекурсии
    if all_samples_same_class(x):
        return LeafNode(majority_class(x))

    if current_depth >= max_depth:
        return LeafNode(most_frequent_class(x))

    # Нахождение оптимального разделения данных
    feature, threshold = find_best_split(x)
    left_subset, right_subset = split_dataset(x, feature, threshold)

    # Рекурсивное построение поддеревьев
    left_subtree = build_decision_tree(left_subset, current_depth + 1)
    right_subtree = build_decision_tree(right_subset, current_depth + 1)

    # Возвращает внутренний узел дерева
    return InternalNode(feature, threshold, left_subtree, right_subtree)
```

### Условия остановки
- Все объекты в вершине принадлежат одному классу
- Достигнута максимальная глубина дерева
- Количество объектов в вершине меньше минимального порога

---

## Критерии разделения

### Функция качества разбиения
Качество разбиения оценивается через уменьшение "хаотичности" данных:

**`Q(x,x_l,x_r) = H(x) - (|x_l|/|x|)H(x_l) - (|x_r|/|x|)H(x_r)`**, где **`H(x)`** — мера хаотичности данных

Чем больше уменьшение хаотичности, тем лучше разбиение.

### Критерий Джини (классификация)
Мера неоднородности данных:

**`H(x) = Σp_c(1 - p_c)`**, где **`p_c`** — доля объектов класса **`c`**

Джини impurity минимальна (равна 0) когда все объекты принадлежат одному классу.

### Энтропия (классификация)
Информационная мера неопределенности:

**`H(x) = -Σp_c·log₂(p_c)`**

Также достигает минимума когда данные чисты.

### MSE (регрессия)
Для задач регрессии используется среднеквадратичная ошибка:

**`H(x) = (1/|x|)Σ(yᵢ - ȳ)²`**

---

## Борьба с переобучением

Деревья решений склонны к переобучению, особенно когда они глубокие. Вот основные методы борьбы с этой проблемой:

### Ограничение глубины дерева
Ограничивает максимальную глубину дерева, предотвращая излишнее усложнение.

Пример:

```python
model = DecisionTreeClassifier(max_depth=10)
```

### Минимальное количество samples в листе
Гарантирует что в каждом листе будет достаточно данных для надежной статистики.

Пример:

```python
model = DecisionTreeClassifier(min_samples_leaf=5)
```

### Минимальное количество samples для разделения
Требует минимального количества объектов в узле для его дальнейшего разделения.

Пример:

```python
model = DecisionTreeClassifier(min_samples_split=20)
```

### Обрезка дерева (pruning)
- Постобработка построенного дерева
- Удаление ветвей, не улучшающих качество на валидации
- Более сложный подход чем предварительные ограничения

---

## Случайный лес

### Основная идея
Случайный лес — это ансамблевый метод, который решает проблему переобучения отдельных деревьев за счет:
- **Баггинга (bootstrap aggregating)**: каждое дерево обучается на случайной подвыборке данных
- **Случайного выбора признаков**: при каждом разделении рассматривается только случайное подмножество признаков

### Алгоритм построения
Пример:

```python
def build_random_forest(x, y, n_estimators, min_samples_leaf, max_features):
    forest = []

    for i in range(n_estimators):
        # Создание бутстрап-выборки
        X_bootstrap, y_bootstrap = create_bootstrap_sample(x, y)

        # Построение дерева с ограничениями
        tree = build_decision_tree(
            X_bootstrap, 
            y_bootstrap,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features
        )
        forest.append(tree)

    return forest
```

### Преимущества случайного леса
- **Устойчивость к переобучению**: усреднение предсказаний многих деревьев
- **Высокая точность**: часто превосходит отдельные деревья
- **Оценка важности признаков**: можно определить наиболее значимые признаки
- **Работа с пропущенными значениями**: более устойчив, чем одно дерево

### Гиперпараметры
- **n_estimators**: количество деревьев
- **max_depth**: максимальная глубина деревьев
- **min_samples_split**: минимальное количество samples для разделения
- **min_samples_leaf**: минимальное количество samples в листе
- **max_features**: количество признаков для рассмотрения в каждом разделении

### Рекомендации по выбору гиперпараметров
- **Для регрессии**: **`max_features = m/3`** (где **`m`** - общее количество признаков)
- **Для классификации**: **`max_features = √m`**
- **n_estimators**: чем больше, тем лучше (до точки насыщения)

---

## Практические аспекты

### Работа с категориальными признаками
1. **One-Hot Encoding**: создание бинарных признаков для каждой категории
2. **Target Encoding**: замена категорий средним значением целевой переменной
3. **Прямая обработка в дереве**: разделение по категориям

### Важность признаков
Случайный лес позволяет оценить важность каждого признака на основе того, насколько он улучшает качество разделения.

Пример:

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(x_train, y_train)
importances = model.feature_importances_
```

### Out-of-Bag оценка (OOB)
- Каждое дерево строится на ~63% данных
- Оставшиеся 37% используются для валидации
- Не требует отдельного validation set

---

## Сравнение с линейными моделями

<div align="center">

<table>
    <tr align="center">
        <th>Характеристика</th>
        <th>Дерево решений</th>
        <th>Линейные модели</th>
    </tr>
    <tr align="center">
        <td>Гибкость</td>
        <td>Высокая, нелинейные зависимости</td>
        <td>Ограниченная, линейные зависимости</td>
    </tr>
    <tr align="center">
        <td>Переобучение</td>
        <td>Склонны к переобучению</td>
        <td>Менее склонны</td>
    </tr>
    <tr align="center">
        <td>Скорость обучения</td>
        <td>Медленнее</td>
        <td>Быстрее (градиентные методы)</td>
    </tr>
    <tr align="center">
        <td>Интерпретируемость</td>
        <td>Высокая (визуализация)</td>
        <td>Высокая (коэффициенты)</td>
    </tr>
    <tr align="center">
        <td>Требования к данным</td>
        <td>Не требует масштабирования</td>
        <td>Требует масштабирования</td>
    </tr>
</table>

</div>

Деревья лучше подходят для сложных нелинейных зависимостей, а линейные модели — когда ожидается линейная связь или важна скорость.

---

## Метрики качества

### Для классификации
- **Accuracy**: общая точность
- **Precision**: точность положительных предсказаний  
- **Recall**: полнота обнаружения положительных классов
- **F1-score**: гармоническое среднее precision и recall
- **ROC-AUC**: площадь под ROC-кривой

### Для регрессии
- **MSE**: средняя квадратичная ошибка
- **MAE**: средняя абсолютная ошибка
- **R²**: коэффициент детерминации

---

## Бутстрап и доверительные интервалы

### Метод бутстрапа
Статистический метод для оценки неопределенности метрик через многократное ресэмплирование данных.

### Реализация бутстрапа
Пример из кода:

```python
boot_accuracies = []
boot_precisions = []
boot_recalls = []
boot_f1_scores = []

n_bootstraps = 1000

for i in range(n_bootstraps):
    if (i + 1) % 100 == 0:
        print(f'Завершено итераций: {i + 1}/{n_bootstraps}')

    x_y_test_boot = x_y_test.sample(len(x_y_test), replace=True)
    x_test_boot = x_y_test_boot.drop(columns='label')
    y_test_boot = x_y_test_boot['label']

    y_pred = best_model.predict(x_test_boot)

    boot_accuracies.append(accuracy_score(y_test_boot, y_pred))
    boot_precisions.append(precision_score(y_test_boot, y_pred, average='weighted',
                                           zero_division=0))
    boot_recalls.append(recall_score(y_test_boot, y_pred, average='weighted',
                                     zero_division=0))
    boot_f1_scores.append(f1_score(y_test_boot, y_pred, average='weighted',
                                   zero_division=0))
```

### Расчет доверительных интервалов
Пример из кода:

```python
def calculate_confidence_interval(metric_values):
    """Вычисляет доверительные интервалы (95%)"""
    sorted_metrics = np.sort(metric_values)
    lower_bound = sorted_metrics[int(0.025 * len(sorted_metrics))]
    upper_bound = sorted_metrics[int(0.975 * len(sorted_metrics))]
    return lower_bound, upper_bound
```

---

## Реализация в проекте

### Дерево решений (Scikit-learn)
Пример:

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Создание и обучение модели
model = DecisionTreeClassifier(
    max_depth=10,
    min_samples_leaf=5,
    random_state=52
)
model.fit(x_train, y_train)

# Предсказание
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
```

### Случайный лес (Scikit-learn)
Пример:

```python
from sklearn.ensemble import RandomForestClassifier

# Создание и обучение модели
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_leaf=5,
    max_features='sqrt',
    random_state=52
)
model.fit(x_train, y_train)

# Важность признаков
importances = model.feature_importances_
```

### Визуализация дерева
Пример:

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(model, feature_names=x.columns, filled=True)
plt.show()
```

---

## Заключение

Деревья решений и случайные леса — мощные инструменты машинного обучения, которые особенно полезны когда важны интерпретируемость модели и работа с нелинейными зависимостями. Случайный лес решает проблему переобучения отдельных деревьев и часто показывает лучшие результаты на практике.
